{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We have a dataset containing images taken from boat for surveillance\n",
    "#3778 noisy images containing six species of fishes and one category that specifies images with no fish\n",
    "#and one category for other fishes\n",
    "#This dataset is taken from www.kaggle.com provided by Nature conservancy, California\n",
    "\n",
    "#Import all the packages needed\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import scipy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from keras.layers import Flatten\n",
    "\n",
    "from keras.constraints import maxnorm\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "#Made a matrix by flattening each image in which each row is a image from the dataset\n",
    "\n",
    "path = 'Desktop/Problem/Images'\n",
    "\n",
    "p = ['ALB','BET','DOL','LAG','NoF','OTHER','SHARK','YFT']\n",
    "L=[]\n",
    "\n",
    "#Made a matrix for each category in the dataset\n",
    "\n",
    "for i in range(8):\n",
    "    listing = os.listdir(path+'/'+p[i])\n",
    "    p[i] = np.array([np.array(cv2.imread(path+'/'+p[i]+'/'+file,0)).flatten()for file in listing])\n",
    "    L.append(len(listing))\n",
    "\n",
    "#Concatenated each matrix into one matrix\n",
    "    \n",
    "    \n",
    "M = np.concatenate((p[0],p[1]))\n",
    "for i in range(2,6):\n",
    "    M = np.concatenate((M,p[i]))\n",
    "label = np.ones(3513,dtype = int)\n",
    "\n",
    "#Labelled each row with values 1 to 8\n",
    "\n",
    "for i in range(1,8):\n",
    "\tlabel[0:1074]=0\n",
    "\tlabel[sum(L[0:i])+1:sum(L[0:i+1])+1] = i\n",
    "print (label)\n",
    "\n",
    "\n",
    "#Shuffled the data for better perfomance\n",
    "\n",
    "data,Label = shuffle(M,label,random_state=2)\n",
    "\n",
    "train_data = [data,Label]\n",
    "\n",
    "X,Y = [train_data[0],train_data[1]]\n",
    "\n",
    "\n",
    "#Splitted the data into Training set, Test set and Validation set\n",
    "\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.4,random_state=4)\n",
    "\n",
    "x_test,x_validation,y_test,y_validation=train_test_split(X_test,Y_test,test_size=0.5,random_state=4)\n",
    "\n",
    "print (X_train.shape)\n",
    "\n",
    "X_train = X_train.reshape((X_train.shape[0],256,256,1))\n",
    "\n",
    "x_validation = x_validation.reshape((x_validation.shape[0],256,256,1))\n",
    "\n",
    "x_test =x_test.reshape((x_test.shape[0],256,256,1))\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "\n",
    "x_validation = x_validation.astype('float32')\n",
    "\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#Normalised each of the pixel values in the training, test and validation sets\n",
    "\n",
    "X_train=X_train/255\n",
    "\n",
    "x_test = x_test / 255\n",
    "\n",
    "x_validation = x_validation/255\n",
    "\n",
    "#Vectorized each label set into a vector of dimension 8\n",
    "\n",
    "Y_train = np_utils.to_categorical(Y_train,8)\n",
    "\n",
    "y_validation = np_utils.to_categorical(y_validation,8)\n",
    "\n",
    "w_test = np_utils.to_categorical(y_test,8)\n",
    "\n",
    "#Constructed the model of Convolutional Neural Network\n",
    "\n",
    "def fish_model():\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(40, 3, 3,  subsample=(2,2), input_shape=(256, 256, 1), activation='relu', border_mode='same'))\n",
    "    model.add(Convolution2D(40, 3, 3, activation='relu', border_mode='same'))\n",
    "    model.add(Convolution2D(40, 3, 3, activation='relu', border_mode='same'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),strides=(1,1)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(6, activation='softmax'))\n",
    "#Kept Stochastic gradient descent algorithm for wait updation\n",
    "    epochs = 5\n",
    "    lrate = 1\n",
    "    decay = lrate/epochs\n",
    "    sgd = SGD(lr=lrate, momentum=0.5, decay=decay, nesterov=False)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['categorical_accuracy'])\n",
    "    return model\n",
    "\n",
    "model= fish_model()\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "#Fit the model to the training set and validation in each epoch\n",
    "\n",
    "history = model.fit(X_train, Y_train, validation_data=(x_validation, y_validation), nb_epoch=16, batch_size=4)\n",
    "print (history.history)\n",
    "scores = model.evaluate(x_test,w_test,verbose=0)\n",
    "print('Test Accuracy:',scores[1]*100)\n",
    "\n",
    "#Predicted for the test dataset \n",
    "\n",
    "prediction=model.predict_proba(x_test, batch_size=4,verbose=1)\n",
    "\n",
    "\n",
    "#Made Confusion matrix, Precison-recall tables\n",
    "p = []\n",
    "for i in range(703):\n",
    "    m=max(prediction[i])\n",
    "    a=list(prediction[i]).index(m)\n",
    "    p.append(a)\n",
    "print(confusion_matrix(y_test,p))\n",
    "print(metrics.classification_report(y_test,p, target_names = ['0','1','2','3','4','5']))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode bytes in position 606-607: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-efb8ffb0f9a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spam.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Taking each line in the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                 \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/favas/anaconda/lib/python3.5/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode bytes in position 606-607: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from collections import defaultdict\n",
    "#import seaborn as sb\n",
    "\n",
    "# defaultdict(int) is exactly like a Python dictionary, but you can specify the default value of every key;\n",
    "# If 'int', then default value is 0; if 'str', default value is ''; if list, default value is []\n",
    "# We need 'int', because we're counting the number for each tag, and starting value is 0.\n",
    "\n",
    "d = defaultdict(int)\n",
    "\n",
    "with open('spam.csv','r') as f:\n",
    "\t# Taking each line in the file\n",
    "\tfor line in f:\n",
    "\t\ttext = TextBlob(line)\n",
    "\n",
    "\t\ttagged_list = text.tags\n",
    "\t\t# Now tagged_list is a list like [('Python', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('high-level', 'JJ'), ('general-purpose', 'JJ'), ('programming', 'NN'), ('language', 'NN')]\n",
    "\n",
    "\t\tfor tuplepair in tagged_list:\n",
    "\t\t\ttag = tuplepair[1]\n",
    "\t\t\td[tag] += 1\n",
    "\n",
    "d = dict(d)\n",
    "#sb.barplot(x=list(d.keys()),y=list(d.values()))\n",
    "\n",
    "# Now, d contains the counts for all POS tags. Now make a barplot using those counts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
